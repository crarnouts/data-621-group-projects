---
title: "DATA 621 Homework #4"
author: "Critical Thinking Group 3"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    toc_depth: 3
    code_folding: "hide"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(tidyverse)
require(gridExtra)
library(Amelia)
library(kableExtra)
library(caret)
library(DMwR)
library(scales)
library(RColorBrewer)
# Thank you Stack Overflow!
# A Prefix nulling hook.

# Make sure to keep the default for normal processing.
default_output_hook <- knitr::knit_hooks$get("output")

# Output hooks handle normal R console output.
knitr::knit_hooks$set( output = function(x, options) {

  comment <- knitr::opts_current$get("comment")
  if( is.na(comment) ) comment <- ""
  can_null <- grepl( paste0( comment, "\\s*\\[\\d?\\]" ),
                     x, perl = TRUE)
  do_null <- isTRUE( knitr::opts_current$get("null_prefix") )
  if( can_null && do_null ) {
    # By default R print output aligns at the right brace.
    align_index <- regexpr( "\\]", x )[1] - 1
    # Two cases: start or newline
    re <- paste0( "^.{", align_index, "}\\]")
    rep <- comment
    x <- gsub( re, rep,  x )
    re <- paste0( "\\\n.{", align_index, "}\\]")
    rep <- paste0( "\n", comment )
    x <- gsub( re, rep,  x )
  }

  default_output_hook( x, options )

})
knitr::opts_template$set("kill_prefix"=list(comment=NA, null_prefix=TRUE))
```

```{r}
df <- read.csv("./data/insurance_training_data.csv") %>%
  mutate(TARGET_FLAG = as.factor(TARGET_FLAG)) %>%
  select(-INDEX)
evaluation <- read.csv("./data/insurance-evaluation-data.csv") %>%
  select(-INDEX)

strip_dollars <- function(x){
  x <- as.character(x)
  x <- gsub(",", "", x)
  x <- gsub("\\$", "", x)
  as.numeric(x)
}

hw4_plot <- function(df, var, strip_dollar_signs = FALSE){
  if(strip_dollar_signs){
    df$X <- strip_dollars(df[[var]])
  } else {
    df$X <- df[[var]]
  }
  plot_1_1 <- ggplot(df, aes(X, TARGET_FLAG, color = TARGET_FLAG)) +
    geom_jitter() +
    scale_color_brewer(palette="Set1") +
    theme_minimal() +
    theme(legend.position = "none",
          axis.title.x = element_blank(),
          axis.title.y = element_blank())
  
  plot_1 <- ggplot(df, aes(X, color = TARGET_FLAG)) +
    geom_density() + 
    scale_color_brewer(palette="Set1") +
    theme_minimal() +
    theme(legend.position = "none",
          axis.title.x = element_blank(),
          axis.title.y = element_blank())
  
  plot_2 <- ggplot(df, aes(X, TARGET_AMT, color = TARGET_FLAG)) +
    geom_point() +
    scale_color_brewer(palette="Set1") +
    theme_minimal() +
    theme(legend.position = "none",
          axis.title.x = element_blank(),
          axis.title.y = element_blank())
  
  grid.arrange(plot_1, plot_2, ncol=2)
}

hw4_categorical_plot <- function(df, var, strip_dollar_signs = FALSE){
  if(strip_dollar_signs){
    df$X <- strip_dollars(df[[var]])
  } else {
    df$X <- as.factor(as.character(df[[var]]))
  }
  plot_1_1 <- ggplot(df, aes(X, TARGET_FLAG, color = TARGET_FLAG)) +
    geom_jitter() +
    scale_color_brewer(palette="Set1") +
    theme_minimal() +
    theme(legend.position = "none",
          axis.title.x = element_blank(),
          axis.title.y = element_blank())
  plot_1 <- df %>%
    group_by(X, TARGET_FLAG) %>%
    tally() %>%
    ggplot(.) +
    geom_bar(aes(X, n, fill = TARGET_FLAG), stat="identity") +
    scale_fill_brewer(palette="Set1") +
    theme_minimal() +
    theme(legend.position = "none",
          axis.title.x = element_blank(),
          axis.title.y = element_blank())
  plot_2 <- df %>%
    filter(TARGET_FLAG == 1) %>%
    ggplot(., aes(X, TARGET_AMT, color = X)) +
      geom_boxplot() +
      scale_color_brewer(palette="Dark2") + #Paired
      theme_minimal() +
      theme(legend.position = "none",
            axis.title.x = element_blank(),
            axis.title.y = element_blank())
  
  grid.arrange(plot_1, plot_2, ncol=2)
}

hw4_target_table <- function(df, var, strip_dollar_signs = FALSE){
  if(strip_dollar_signs){
    df$X <- strip_dollars(df[[var]])
  } else {
    df$X <- df[[var]]
  }
  df %>%
    group_by(X, TARGET_FLAG) %>%
    tally() %>%
    spread(TARGET_FLAG, n) %>%
    rename(Variable = X) %>%
    kable() %>%
    kable_styling()
}
```

## Introduction 

We have been given a dataset with `r nrow(df)` records representing customers of an auto insurance company.  Each record has two response variables.  The first is a binary flag where one means a person was in a car crash and zero means they were not.  There are `r df %>% filter(TARGET_FLAG == 1) %>% nrow()` rows with a one flag and `r df %>% filter(TARGET_FLAG == 0) %>% nrow()` with a zero flag.

```{r, echo=FALSE}
df %>% 
  ggplot(aes(x=TARGET_FLAG,fill=TARGET_FLAG)) +
  geom_bar() + scale_y_continuous() + scale_fill_brewer(palette="Set1") +
  theme_light() +
  theme(legend.position = "none") +
  labs(x="TARGET_FLAG", y="# Observations")
```

The first objective is to train a logistic regression classifier to predict if a person was in a car crash.  The second reponse variable is the amount it will cost if the person crashes their car.  The value is zero if the person did not crash their car.

The second objective will be to train a regression model to predict the cost of a crash, if one occurred.  

```{r, echo=FALSE}
df %>% filter(TARGET_FLAG == 1) %>%
  ggplot(aes(x=TARGET_AMT)) + geom_density() +
  geom_vline(aes(xintercept = mean(TARGET_AMT)), lty=2, col="red") +
  geom_label(aes(x=mean(TARGET_AMT),y=1,label="mu"),parse=T) +
  geom_vline(aes(xintercept = median(TARGET_AMT)), lty=2, col="darkgreen") +
  geom_label(aes(x=median(TARGET_AMT),y=.5,label="median")) +
  scale_x_log10(labels=comma) + theme_light() +
  labs(title="TARGET_AMT Density Plot", caption="x-axis is log 10 scale",
       y="Density", x="LOG(TARGET_AMT)")
```

Looking at the distribution of the `TARGET_AMT` variable, we can see that the variable is considerably right-skewed. Thus, a LOG transform might be best here.

## Data Exploration

We will first look at the summary statistics for the data

```{r}
df %>%
  summary() %>%
  kable() %>%
  kable_styling()
```

There are some missing values that we will need to deal with.  Let's look at the predictors to see how well they predict if there is an accident (plot on the left) and how much it would cost (plot on the right).  Numeric variables will get scatterplots, while categorical will get box plots

### AGE

**Theoretical Effect**
Very young people tend to be risky.  Maybe very old people also.

```{r} 
hw4_plot(df, "AGE")
```

**Observed Effect**
There is very little information encapsulated in the age of the person.


### BLUEBOOK

**Theoretical Effect**
Unknown effect on probability of collision, but probably effects the payout if there is a crash.

```{r} 
hw4_plot(df, "BLUEBOOK", TRUE)
```

**Observed Effect**
The plot suggests that people with low value and high value vehicles are equally likely to be in a crash.  It also suggests the payout is not strongly correlated with the vehicle's value.

### CAR_AGE

**Theoretical Effect**
Unknown effect on probability of collision, but probably effects the payout if there is a crash.

```{r} 
hw4_plot(df, "CAR_AGE")
```

**Observed Effect**
The plot suggests that people with old or new vehicles are equally likely to be in a crash.  It also suggests the payout is not strongly correlated with the vehicle's age.  *There is an observation with a negative value that will need to be cleaned up.*

### CAR_TYPE

**Theoretical Effect**
Unknown effect on probability of collision, but probably effects the payout if there is a crash.

```{r} 
hw4_categorical_plot(df, "CAR_TYPE")
hw4_target_table(df, "CAR_TYPE")
```

**Observed Effect**
The plot suggests that the type of vehicle has no effect on the probability of being in an accident, and little effect of the cost.

### CAR_USE

**Theoretical Effect**
Commercian vehicles are driven more, so might increase probability of collision.

```{r} 
hw4_categorical_plot(df, "CAR_USE")
hw4_target_table(df, "CAR_USE")
```

**Observed Effect**
The plot suggests that the use does not increase or decrease the probability of being in an accident or the amount of the claim.

### CLM_FREQ

**Theoretical Effect**
The more claims you filed in the past, the more you are likely to file in the future.

```{r} 
hw4_categorical_plot(df, "CLM_FREQ")
hw4_target_table(df, "CLM_FREQ")
```

**Observed Effect**
Looks like there are few people who signed up for the frequent accident club.  All jokes aside, the theoretical effect is not observed in the data.

### EDUCATION

**Theoretical Effect**
Unknown effect, but in theory more educated people tend to drive more safely

```{r} 
hw4_categorical_plot(df, "EDUCATION")
hw4_target_table(df, "EDUCATION")
```

**Observed Effect**
The theoretical effect has no basis and is somewhat elitist.


### HOMEKIDS

**Theoretical Effect**
Unknown effect

```{r} 
hw4_categorical_plot(df, "HOMEKIDS")
hw4_target_table(df, "HOMEKIDS")
```

**Observed Effect**
It's interesting to note that those with kids have a 0.34 probability of being in an accident.  Those without kids have a 0.22 probability of being in an accident.  This may be useful.

### HOME_VAL

**Theoretical Effect**
In theory, home owners tend to drive more responsibly.

```{r} 
hw4_plot(df, "HOME_VAL", TRUE)
```

**Observed Effect**
There are a lot of zero values.  I am assuming that it is a zero because they are renters.

### INCOME

**Theoretical Effect**
In theory, rich people trend to get into fewer crashes.

```{r} 
hw4_plot(df, "INCOME", TRUE)
```

**Observed Effect**
There are a lot of zero values.  I am assuming that it is a zero because they are renters.


### JOB

**Theoretical Effect**
In theory, white collar jobs tend to be safer

```{r} 
hw4_categorical_plot(df, "JOB")
hw4_target_table(df, "JOB")
```

**Observed Effect**
There is a 0.21 probability for white collar job holders being in an accident.  there's a 0.34 probability for the blue collar jobs.  There may be some truth to the theoretical effect.


### KIDSDRIV

**Theoretical Effect**
When teenagers drive your car, ou are more likely to get into crashes

```{r} 
hw4_categorical_plot(df, "KIDSDRIV")
hw4_target_table(df, "KIDSDRIV")
```

**Observed Effect**
INSERT TEXT HERE

### MSTATUS

**Theoretical Effect**
In theory, married people drive more safely

```{r} 
hw4_categorical_plot(df, "MSTATUS")
hw4_target_table(df, "MSTATUS")
```

**Observed Effect**
INSERT TEXT HERE

### MVR_PTS

**Theoretical Effect**
If you get lots of traffic tickets, you trend to get into more crashes

```{r} 
hw4_categorical_plot(df, "MVR_PTS")
hw4_target_table(df, "MVR_PTS")
```

**Observed Effect**
INSERT TEXT HERE


Looking at the data, we can see that there are some missing values:

```{r}
missmap(df, main = "Missing vs Observed Values")
```

Specifically, there `r df %>% filter(is.na(CAR_AGE)) %>% nrow()` observations where the `CAR_AGE` variable are missing, `r df %>% filter(is.na(YOJ)) %>% nrow()` observations where variable `YOJ` is missing, but only `r df %>% filter(is.na(YOJ),is.na(CAR_AGE)) %>% nrow()` observations missing both of these varables. So, if imputation is not viable here, we will lose as many as 488 observations out of the 8161 in our training set (or about 6%).

## Data Preparation

```{r}
df %>%
  summary() %>%
  kable() %>%
  kable_styling()
```

### Fix Data Types

There are some variables that are currently factors that are dollar values that need to be transformed into numeric variables.  We will do this to both the `df` and `evaluation` data frames.

```{r}
strip_dollars <- function(x){
  x <- as.character(x)
  x <- gsub(",", "", x)
  x <- gsub("\\$", "", x)
  as.numeric(x)
}

fix_data_types <- function(messy_df){
  messy_df %>%
    rowwise() %>%
    mutate(INCOME = strip_dollars(INCOME),
           HOME_VAL = strip_dollars(HOME_VAL),
           BLUEBOOK = strip_dollars(BLUEBOOK),
           OLDCLAIM = strip_dollars(OLDCLAIM)) %>%
    ungroup()
}

df <- fix_data_types(df)
evaluation <- fix_data_types(evaluation)
```

Now that we have fixed the variables, we can look at a sumamry of the data:

```{r}
df %>%
  summary() %>%
  kable() %>%
  kable_styling()
```

### Fix Missing Values
There are missing values.  We will fill in the missing variables using KNN.

```{r, null_prefix = TRUE}
# set.seed(42)
# knn <- df %>%
#   select(-TARGET_FLAG, -TARGET_AMT, is.numeric()) %>%
#   #na.omit() %>%
#   knnImputation()
# 
for(var in names(df)){
  impute_me <- is.na(df[[var]])
  if (nrow(df[impute_me,]) > 0){
    print(paste("Fixing the", nrow(df[impute_me,]), "missing values in", var, "(df)"))
    #df[impute_me, var] <- knn[impute_me, var] 
  }
}


## adding in some imputing logic
library(VIM)
library(laeken)


df[,3:ncol(df)] <- df[,3:ncol(df)] %>% kNN(numFun = weightedMean, weightDist=TRUE)
```




### Feature Creation

We will create a log transformed income and home value feature.  We will also create an average claim amount that will hopefully track better with the `TARGET_AMT` variable.

```{r}
feature_creation <- function(d){
  d %>%
    rowwise() %>%
    mutate(LOG_INCOME = log(INCOME + 1),
           LOG_HOME_VAL = log(HOME_VAL + 1),
           AVG_CLAIM = ifelse(CLM_FREQ > 0, OLDCLAIM / CLM_FREQ, 0)) %>%
    ungroup()
}

df <- feature_creation(df)
evaluation <- feature_creation(evaluation)

df$TARGET_FLAG_BOOL <- ifelse(df$TARGET_FLAG=="1",1,0)
```

### Creating Training/Test Data Sets

Now that we have a complete data set we will split the data into a training (`train`) and test set (`test`). We'll use a 70-30 split between train and test, respectively.

```{r}
set.seed(42)
train_index <- createDataPartition(df$TARGET_AMT, p = .7, list = FALSE, times = 1)
train <- df[train_index,]
test <- df[-train_index,]
```



```{r}
library(lme4)
glm <- glm(TARGET_AMT ~ .-TARGET_FLAG-TARGET_FLAG_BOOL , data= train, family=poisson) 
summary(glm)

test$Prediction =predict.glm(glm, newdata=test, type="response")

cor(test$TARGET_AMT, test$Prediction)

glm2 <- glm(TARGET_AMT ~ .-TARGET_FLAG-TARGET_FLAG_BOOL , data= train) 
summary(glm2)

test$Prediction2 =predict.glm(glm2, newdata=test, type="response")

cor(test$TARGET_AMT, test$Prediction)


glm3 <- glm(TARGET_AMT ~ AGE+PARENT1+MSTATUS+JOB+BLUEBOOK+TIF+CAR_AGE_density+URBANICITY+MVR_PTS+REVOKED+CLM_FREQ+OLDCLAIM , data= train, family=poisson) 
summary(glm3)

test$Prediction =predict.glm(glm3, newdata=test, type="response")

cor(test$TARGET_AMT, test$Prediction)
```




```{r}
source("https://raw.githubusercontent.com/crarnouts/Corey-s_Scripts_For_Reference/master/11_5_19_Density_Diff_Function")
```


### Creating Some New Variables
      
```{r}


## try it out 
train <- density_diff(train,test,"INCOME",train$TARGET_FLAG_BOOL,"income_density")$new_data_training
test <- density_diff(train,test,"INCOME",train$TARGET_FLAG_BOOL,"income_density")$new_data_eval

density_diff(train,test,"INCOME",train$TARGET_FLAG_BOOL,"income_density")$dist_diff
density_diff(train,test,"INCOME",train$TARGET_FLAG_BOOL,"income_density")$dist


train <- density_diff(train,test,"CAR_AGE",train$TARGET_FLAG_BOOL,"CAR_AGE_density")$new_data_training
test <- density_diff(train,test,"CAR_AGE",train$TARGET_FLAG_BOOL,"CAR_AGE_density")$new_data_eval

density_diff(train,test,"CAR_AGE",train$TARGET_FLAG_BOOL,"CAR_AGE_density")$dist_diff
density_diff(train,test,"CAR_AGE",train$TARGET_FLAG_BOOL,"CAR_AGE_density")$dist

train <- density_diff(train,test,"TRAVTIME",train$TARGET_FLAG_BOOL,"TRAVTIME_density")$new_data_training
test <- density_diff(train,test,"TRAVTIME",train$TARGET_FLAG_BOOL,"TRAVTIME_density")$new_data_eval

density_diff(train,test,"TRAVTIME",train$TARGET_FLAG_BOOL,"TRAVTIME_density")$dist_diff
density_diff(train,test,"TRAVTIME",train$TARGET_FLAG_BOOL,"TRAVTIME_density")$dist


```

##Test out Some Models

```{r}

```





