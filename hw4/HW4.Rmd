---
title: "DATA 621 Homework #4"
author: "Critical Thinking Group 3"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    toc_depth: 3
    code_folding: "hide"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(tidyverse)
require(gridExtra)
library(Amelia)
library(kableExtra)
library(caret)
library(DMwR)
library(scales)
library(RColorBrewer)
# Thank you Stack Overflow!
# A Prefix nulling hook.

# Make sure to keep the default for normal processing.
default_output_hook <- knitr::knit_hooks$get("output")

# Output hooks handle normal R console output.
knitr::knit_hooks$set( output = function(x, options) {

  comment <- knitr::opts_current$get("comment")
  if( is.na(comment) ) comment <- ""
  can_null <- grepl( paste0( comment, "\\s*\\[\\d?\\]" ),
                     x, perl = TRUE)
  do_null <- isTRUE( knitr::opts_current$get("null_prefix") )
  if( can_null && do_null ) {
    # By default R print output aligns at the right brace.
    align_index <- regexpr( "\\]", x )[1] - 1
    # Two cases: start or newline
    re <- paste0( "^.{", align_index, "}\\]")
    rep <- comment
    x <- gsub( re, rep,  x )
    re <- paste0( "\\\n.{", align_index, "}\\]")
    rep <- paste0( "\n", comment )
    x <- gsub( re, rep,  x )
  }

  default_output_hook( x, options )

})
knitr::opts_template$set("kill_prefix"=list(comment=NA, null_prefix=TRUE))
```

```{r}
df <- read.csv("./data/insurance_training_data.csv") %>%
  mutate(TARGET_FLAG = as.factor(TARGET_FLAG)) %>%
  select(-INDEX)
evaluation <- read.csv("./data/insurance-evaluation-data.csv") %>%
  select(-INDEX)

strip_dollars <- function(x){
  x <- as.character(x)
  x <- gsub(",", "", x)
  x <- gsub("\\$", "", x)
  as.numeric(x)
}

hw4_plot <- function(df, var, strip_dollar_signs = FALSE){
  if(strip_dollar_signs){
    df$X <- strip_dollars(df[[var]])
  } else {
    df$X <- df[[var]]
  }
  plot_1 <- ggplot(df, aes(X, TARGET_FLAG, color = TARGET_FLAG)) +
    geom_jitter() +
    theme_minimal() +
    theme(legend.position = "none",
          axis.title.x = element_blank(),
          axis.title.y = element_blank())
  
  plot_2 <- ggplot(df, aes(X, TARGET_AMT, color = TARGET_FLAG)) +
    geom_point() +
    theme_minimal() +
    theme(legend.position = "none",
          axis.title.x = element_blank(),
          axis.title.y = element_blank())
  
  grid.arrange(plot_1, plot_2, ncol=2)
}

hw4_categorical_plot <- function(df, var, strip_dollar_signs = FALSE){
  if(strip_dollar_signs){
    df$X <- strip_dollars(df[[var]])
  } else {
    df$X <- df[[var]]
  }
  plot_1 <- ggplot(df, aes(X, TARGET_FLAG, color = TARGET_FLAG)) +
    geom_jitter() +
    theme_minimal() +
    theme(legend.position = "none",
          axis.title.x = element_blank(),
          axis.title.y = element_blank())
  
  plot_2 <- df %>%
    filter(TARGET_FLAG == 1) %>%
    ggplot(., aes(X, TARGET_AMT)) +
      geom_boxplot() +
      theme_minimal() +
      theme(legend.position = "none",
            axis.title.x = element_blank(),
            axis.title.y = element_blank())
  
  grid.arrange(plot_1, plot_2, ncol=2)
}
```

## Introduction 

We have been given a dataset with `r nrow(df)` records representing customers of an auto insurance company.  Each record has two response variables.  The first is a binary flag where one means a person was in a car crash and zero means they were not.  There are `r df %>% filter(TARGET_FLAG == 1) %>% nrow()` rows with a one flag and `r df %>% filter(TARGET_FLAG == 0) %>% nrow()` with a zero flag.

```{r, echo=FALSE}
df %>% 
  ggplot(aes(x=TARGET_FLAG,fill=TARGET_FLAG)) +
  geom_bar() + scale_y_continuous() + scale_fill_brewer(palette="Set1") +
  theme_light() +
  theme(legend.position = "none") +
  labs(x="TARGET_FLAG", y="# Observations")
```

The first objective is to train a logistic regression classifier to predict if a person was in a car crash.  The second reponse variable is the amount it will cost if the person crashes their car.  The value is zero if the person did not crash their car.

The second objective will be to train a regression model to predict the cost of a crash, if one occurred.  

```{r, echo=FALSE}
df %>% filter(TARGET_FLAG == 1) %>%
  ggplot(aes(x=TARGET_AMT)) + geom_density() +
  geom_vline(aes(xintercept = mean(TARGET_AMT)), lty=2, col="red") +
  geom_label(aes(x=mean(TARGET_AMT),y=1,label="mu"),parse=T) +
  geom_vline(aes(xintercept = median(TARGET_AMT)), lty=2, col="darkgreen") +
  geom_label(aes(x=median(TARGET_AMT),y=.5,label="median")) +
  scale_x_log10(labels=comma) + theme_light() +
  labs(title="TARGET_AMT Density Plot", caption="x-axis is log 10 scale",
       y="Density", x="LOG(TARGET_AMT)")
```

Looking at the distribution of the `TARGET_AMT` variable, we can see that the variable is considerably right-skewed. Thus, a LOG transform might be best here.

## Data Exploration

We will first look at the summary statistics for the data

```{r}
df %>%
  summary() %>%
  kable() %>%
  kable_styling()
```

There are some missing values that we will need to deal with.  Let's look at the predictors to see how well they predict if there is an accident (plot on the left) and how much it would cost (plot on the right).  Numeric variables will get scatterplots, while categorical will get box plots

### AGE

**Theoretical Effect**
Very young people tend to be risky.  Maybe very old people also.

```{r} 
hw4_plot(df, "AGE")
```

**Observed Effect**
There is very little information encapsulated in the age of the person.


### BLUEBOOK

**Theoretical Effect**
Unknown effect on probability of collision, but probably effects the payout if there is a crash.

```{r} 
hw4_plot(df, "BLUEBOOK", TRUE)
```

**Observed Effect**
The plot suggests that people with low value and high value vehicles are equally likely to be in a crash.  It also suggests the payout is not strongly correlated with the vehicle's value.

### CAR_AGE

**Theoretical Effect**
Unknown effect on probability of collision, but probably effects the payout if there is a crash.

```{r} 
hw4_plot(df, "CAR_AGE")
```

**Observed Effect**
The plot suggests that people with old or new vehicles are equally likely to be in a crash.  It also suggests the payout is not strongly correlated with the vehicle's age.  *There is an observation with a negative value that will need to be cleaned up.*

### CAR_TYPE

**Theoretical Effect**
Unknown effect on probability of collision, but probably effects the payout if there is a crash.

```{r} 
hw4_categorical_plot(df, "CAR_TYPE")
```

**Observed Effect**
The plot suggests that the type of vehicle has no effect on the probability of being in an accident, and little effect of the cost.

Looking at the data, we can see that there are some missing values:

```{r}
missmap(df, main = "Missing vs Observed Values")
```

Specifically, there `r df %>% filter(is.na(CAR_AGE)) %>% nrow()` observations where the `CAR_AGE` variable are missing, `r df %>% filter(is.na(YOJ)) %>% nrow()` observations where variable `YOJ` is missing, but only `r df %>% filter(is.na(YOJ),is.na(CAR_AGE)) %>% nrow()` observations missing both of these varables. So, if imputation is not viable here, we will lose as many as 488 observations out of the 8161 in our training set (or about 6%).

## Data Preparation

```{r}
df %>%
  summary() %>%
  kable() %>%
  kable_styling()
```

### Fix Data Types

There are some variables that are currently factors that are dollar values that need to be transformed into numeric variables.  We will do this to both the `df` and `evaluation` data frames.

```{r}
strip_dollars <- function(x){
  x <- as.character(x)
  x <- gsub(",", "", x)
  x <- gsub("\\$", "", x)
  as.numeric(x)
}

fix_data_types <- function(messy_df){
  messy_df %>%
    rowwise() %>%
    mutate(INCOME = strip_dollars(INCOME),
           HOME_VAL = strip_dollars(HOME_VAL),
           BLUEBOOK = strip_dollars(BLUEBOOK),
           OLDCLAIM = strip_dollars(OLDCLAIM)) %>%
    ungroup()
}

df <- fix_data_types(df)
evaluation <- fix_data_types(evaluation)
```

Now that we have fixed the variables, we can look at a sumamry of the data:

```{r}
df %>%
  summary() %>%
  kable() %>%
  kable_styling()
```

### Fix Missing Values
There are missing values.  We will fill in the missing variables using KNN.

```{r, null_prefix = TRUE}
# set.seed(42)
# knn <- df %>%
#   select(-TARGET_FLAG, -TARGET_AMT, is.numeric()) %>%
#   #na.omit() %>%
#   knnImputation()
# 
# for(var in names(df)){
#   impute_me <- is.na(df[[var]])
#   if (nrow(df[impute_me,]) > 0){
#     print(paste("Fixing the", nrow(df[impute_me,]), "missing values in", var, "(df)"))
#     #df[impute_me, var] <- knn[impute_me, var] 
#   }
# }
```


### Feature Creation

We will create a log transformed income and home value feature.  We will also create an average claim amount that will hopefully track better with the `TARGET_AMT` variable.

```{r}
feature_creation <- function(d){
  d %>%
    rowwise() %>%
    mutate(LOG_INCOME = log(INCOME + 1),
           LOG_HOME_VAL = log(HOME_VAL + 1),
           AVG_CLAIM = ifelse(CLM_FREQ > 0, OLDCLAIM / CLM_FREQ, 0)) %>%
    ungroup()
}

df <- feature_creation(df)
evaluation <- feature_creation(evaluation)
```

### Creating Training/Test Data Sets

Now that we have a complete data set we will split the data into a training (`train`) and test set (`test`). We'll use a 70-30 split between train and test, respectively.

```{r}
set.seed(42)
train_index <- createDataPartition(df$TARGET_AMT, p = .7, list = FALSE, times = 1)
train <- df[train_index,]
test <- df[-train_index,]
```