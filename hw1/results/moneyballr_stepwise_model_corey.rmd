---
title: "MoneyBallR_Modeling"
author: "Corey Arnouts"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    toc_depth: 3
    code_folding: "hide"
  html_document:
    df_print: paged
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, comment = NA)
```

```{r}
set.seed(1234)
library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)
```


# Decision Trees to Find Variable Interactions
```{r}
library(tidyverse)
df_train <- read.csv("https://raw.githubusercontent.com/crarnouts/Data_621/master/moneyball-training-data.csv")
df_test <- read.csv("https://raw.githubusercontent.com/crarnouts/Data_621/master/moneyball-evaluation-data.csv")
source("https://raw.githubusercontent.com/crarnouts/Data_605_Final/master/RandomForestNulls_testing.R")
colnames(df_train) <- sub("TEAM_", "", colnames(df_train))
colnames(df_test) <- sub("TEAM_", "", colnames(df_test))
df_test <- RF_with_Nulls(df_train,df_test,"TARGET_WINS",.5,10,25,.02,5,1)
## Mike Silva Added this line
df_train$BATTING_1B <- df_train$BATTING_H - df_train$BATTING_2B - df_train$BATTING_3B - df_train$BATTING_HR
```

# See if we can impute some misssing values Using a Random Forest
```{r}
library(randomForest)
df_train.na <- df_train
df_train.imputed <-rfImpute(TARGET_WINS ~.,df_train.na)
str(df_train)
str(df_train.imputed)
df_train <- df_train.imputed
```



# Correlation Matrix
```{r}
na_count <-sapply(df_train, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
library(corrplot)
M <- cor(df_train)
corrplot(M, method = "circle") #plot matrix
library("PerformanceAnalytics")
chart.Correlation(df_train, histogram=TRUE, pch=19)
```



# Principal Components
```{r}
#PCA Analysis in R DataCamp Article
library(tidyverse)
mtcars.pca <- prcomp(mtcars[,c(1:7,10,11)], center = TRUE,scale. = TRUE) #exclude the categorical variables
mtcars <- mtcars 
summary(mtcars.pca)
str(mtcars.pca)
library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)
ggbiplot(mtcars.pca) # this shows you what attributes contribute to principal component 1 and 2
ggbiplot(mtcars.pca, labels=rownames(mtcars)) # add in the sample information
mtcars.country <- c(rep("Japan", 3), rep("US",4), rep("Europe", 7),rep("US",3), "Europe", rep("Japan", 3), rep("US",4), rep("Europe", 3), "US", rep("Europe", 3))
ggbiplot(mtcars.pca,ellipse=TRUE,  labels=rownames(mtcars), groups=mtcars.country) #represent circles around groupings I want to do this binary classifiers
ggbiplot(mtcars.pca,ellipse=TRUE,choices=c(3,4),   labels=rownames(mtcars), groups=mtcars.country) # take a look at some of the other principal components
################################################################################
##################### Same thing but with MoneyBallR data ######################
################################################################################
df_train.pca <- prcomp(df_train[,c(3:17)], center = TRUE,scale. = TRUE) #exclude the categorical variables
df_train.target <- 
summary(df_train.pca)
str(df_train.pca)
ggbiplot(df_train.pca) + geom_point(size = .01)+ xlim(-8,8)+ ylim(-10,5)
ggbiplot(mtcars.pca,ellipse=TRUE,choices=c(3,4),   labels=rownames(mtcars), groups=mtcars.country) # take a look at some of the other principal components
library(tidyverse)
test <- df_train %>% dplyr::select(BATTING_H)
df_train.pca <- df_train %>% dplyr::select(PITCHING_HR,BATTING_HR) %>%  prcomp(center = TRUE,scale. = TRUE) #exclude the categorical variables
summary(df_train.pca)
str(df_train.pca)
library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)
ggbiplot(df_train.pca) + geom_point(size = .01)+ xlim(-8,8)+ ylim(-10,5)
ggplot(df_train, aes(x=PITCHING_HR, y=BATTING_HR, colour = TARGET_WINS)) +
    geom_point(shape=1) + 
scale_colour_gradient(low="cyan", high="firebrick1")     # Use hollow circles
df_train$HR_RATIO <- df_train$BATTING_HR/df_train$PITCHING_HR
library(plotly)
ggplotly(ggplot(df_train, aes(x=PITCHING_HR, y=HR_RATIO, colour = TARGET_WINS)) +
    geom_point(shape=1) + 
scale_colour_gradient(low="cyan", high="firebrick1")     # Use hollow circles
)
mid <- mean(df_train$TARGET_WINS)
ggplotly(ggplot(df_train, aes(x=BATTING_H, y=PITCHING_H, colour = TARGET_WINS)) +
    geom_point(shape=1) + 
 scale_color_gradient2(midpoint = mid, low = "blue", mid = "white",
                            high = "red", space = "Lab" )     # Use hollow circles
)
## look at batting hits to pitching hits ratio
df_train$Hitting_Ratio <- df_train$BATTING_H/df_train$PITCHING_H
ggplotly(ggplot(df_train, aes(x=Hitting_Ratio, y=TARGET_WINS, colour = TARGET_WINS)) +
    geom_point(shape=1) + 
 scale_color_gradient2(midpoint = mid, low = "blue", mid = "white",
                            high = "red", space = "Lab" )     # Use hollow circles
)
for (i in 1:ncol(df_train)){
ggplotly(ggplot(df_train, aes(x=df_train[[i]], y=TARGET_WINS, colour = TARGET_WINS)) +
    geom_point(shape=1) + 
 scale_color_gradient2(low = "blue",
                            high = "red", space = "Lab" )     # Use hollow circles
)
}
ggplotly(ggplot(df_train, aes(x=BATTING_2B , y=BATTING_H, colour = TARGET_WINS)) +
    geom_point(shape=1) + 
 scale_color_gradient2(midpoint = mid, low = "blue", mid = "white",
                            high = "red", space = "Lab" ) )
```


# Predict Hits Batted by Hits Pitched and then see what the difference is so like hits batted above expectation
And then use this as a predictive feature

```{r eval=FALSE}
#First Fit Target Wins just by Batted_H and Pitched_H
model <- lm(TARGET_WINS ~ BATTING_H + PITCHING_H, data = df_train)
df_train$Prediction <- predict(model,df_train)
summary(model)
## .45 correlation 
##try on it on out of sample
### Add my meta features ###
df_train <- dplyr::filter(df_train,df_train$TARGET_WINS != 0) %>% filter(BATTING_H != 0) %>% filter(BATTING_3B != 0) %>% 
filter(BATTING_2B != 0)%>% filter(BATTING_HR != 0)%>% filter(BATTING_BB != 0)%>% filter(BATTING_SO != 0)
# ## Adding in some new features ##
 df_train$Pitch_HR_under_2 <- ifelse(df_train$PITCHING_HR <2,1,0)
 df_train$Errors_over_1215 <- ifelse(df_train$FIELDING_E > 1215,1,0)
 df_train$HR_RATIO_2 <- (df_train$HR_RATIO)^2
### Add in some new feature engineering 
df_train$BATTING_1B <- df_train$BATTING_H - df_train$BATTING_2B - df_train$BATTING_3B - df_train$BATTING_HR
df_train$BASES_ACQUIRED <- df_train$BATTING_1B + 2*df_train$BATTING_2B + 3*df_train$BATTING_3B + 4*df_train$BATTING_H + df_train$BATTING_BB
train.index1 <- createDataPartition(df_train$TARGET_WINS, p = .7, list = FALSE)
train_data<- df_train[ train.index1,]
hold_out_data  <- df_train[-train.index1,]
model <- lm(TARGET_WINS ~ ., data = train_data)
hold_out_data$Prediction <- predict(model,hold_out_data)
cor(hold_out_data$Prediction,hold_out_data$TARGET_WINS)
summary(model)
## Non linear benchmark random forest
train.index1 <- createDataPartition(df_train$TARGET_WINS, p = .7, list = FALSE)
train_data<- df_train[ train.index1,]
hold_out_data  <- df_train[-train.index1,]
hold_out_data <- RF_with_Nulls(df_train,hold_out_data,"TARGET_WINS",.5,4,500,.005,5,10)
cor(hold_out_data$prediction_overall,hold_out_data$TARGET_WINS)
# 
# 
# ############# FITTING THE MODEL WITH EVERYTHING ###################
# ############# BASELINE ############################################
# df_train$INDEX <- NULL
# train.index1 <- createDataPartition(df_train$TARGET_WINS, p = .7, list = FALSE)
# train_data<- df_train[ train.index1,]
# hold_out_data  <- df_train[-train.index1,]
# model <- lm(TARGET_WINS ~ ., data = train_data)
# hold_out_data$Prediction <- predict(model,hold_out_data)
# cor(hold_out_data$Prediction,hold_out_data$TARGET_WINS)
# summary(model)
ggplotly(ggplot(hold_out_data, aes(x=Prediction, y=TARGET_WINS, colour = TARGET_WINS)) +
     geom_point(shape=1) + 
  scale_color_gradient2(low = "blue",
                             high = "red", space = "Lab" )     # Use hollow circles
 )
ggplot(data = hold_out_data, aes(x = Prediction, y = TARGET_WINS)) + 
  geom_point(color='blue') +
  geom_smooth(method = "lm", se = FALSE)
toselect.x <- summary(model)$coeff[-1,4] < 0.05 #
relevant.x <- names(toselect.x)[toselect.x == TRUE] 
 
sig.formula <- as.formula(paste("TARGET_WINS ~",paste(relevant.x, collapse= "+")))
   
model <- lm(formula = sig.formula, data = train_data)
summary(model)
```


## Look at Variance Inflation Factors 
```{r}
df_train1 <- df_train
df_train1$BATTING_H <- NULL
df_train1$BATTING_SO<- NULL
df_train1$HR_RATIO <- NULL
df_train1$BASES_ACQUIRED <- NULL
df_train1$Hitting_Ratio <- NULL
df_train1$PITCHING_HR <- NULL
for (i in 3:ncol(df_train1)){
col <- noquote(paste(colnames(df_train1)[i],"~ ."))
model <- lm(col, data = df_train1)
r_squared <- summary(model)$adj.r.squared
VIF <- 1/(1-(r_squared))
print(colnames(df_train1)[i])
print(VIF)
#print(summary(model))
}
```

```{r}
train.index1 <- createDataPartition(df_train1$TARGET_WINS, p = .7, list = FALSE)
train_data<- df_train[ train.index1,]
hold_out_data  <- df_train[-train.index1,]
model <- lm(TARGET_WINS ~ BATTING_BB + BATTING_SO + BASERUN_CS + PITCHING_HR + 
    PITCHING_BB + PITCHING_SO + FIELDING_E + FIELDING_DP + I(BATTING_2B^2) + 
    I(BATTING_3B^2) + I(BATTING_BB^2) + I(BATTING_SO^2) + I(BASERUN_SB^2) + 
    I(BASERUN_CS^2) + I(PITCHING_H^2) + I(PITCHING_BB^2) + I(PITCHING_SO^2) + 
    I(FIELDING_E^2) + I(FIELDING_DP^2) + I(BATTING_1B^2) + 
    I(BATTING_2B^3) + I(BATTING_3B^3) + I(BATTING_HR^3) + I(BATTING_SO^3) + 
    I(BASERUN_CS^3) + I(PITCHING_H^3) + I(PITCHING_BB^3) + I(FIELDING_E^3) + 
    I(FIELDING_DP^3) + I(BATTING_2B^4) + I(BATTING_3B^4) + I(BATTING_HR^4) + 
    I(BATTING_BB^4) + I(BATTING_SO^4) + I(BASERUN_CS^4) + I(PITCHING_H^4) + 
    I(PITCHING_BB^4) + I(FIELDING_E^4) + I(FIELDING_DP^4) + I(BATTING_1B^4), data = df_train)
hold_out_data$Prediction <- predict(model,hold_out_data)
cor(hold_out_data$Prediction,hold_out_data$TARGET_WINS)
summary(model)
```

# Poisson GLMS
```{r}
df_train$INDEX <- NULL
train.index1 <- createDataPartition(df_train1$TARGET_WINS, p = .7, list = FALSE)
train_data<- df_train[ train.index1,]
hold_out_data  <- df_train[-train.index1,]
# Poisson Regression
# where count is a count and
# x1-x3 are continuous predictors
fit <- glm(TARGET_WINS ~ ., data = train_data, family=poisson())
summary(fit)
hold_out_data$Prediction <- predict(fit,hold_out_data)
cor(hold_out_data$Prediction,hold_out_data$TARGET_WINS)
```


# Stepwise Regression
```{r}
set.seed(1234)
train.index1 <- createDataPartition(df_train$TARGET_WINS, p = .7, list = FALSE)
train_data<- df_train[ train.index1,]
hold_out_data  <- df_train[-train.index1,]


full_formula <- "TARGET_WINS ~ BATTING_2B + BATTING_3B + BATTING_HR + BATTING_BB + BATTING_SO + BASERUN_SB + BASERUN_CS + PITCHING_H + PITCHING_HR + PITCHING_BB + PITCHING_SO + FIELDING_E + FIELDING_DP + BATTING_1B + I(BATTING_2B^2) + I(BATTING_3B^2) + I(BATTING_HR^2) + I(BATTING_BB^2) + I(BATTING_SO^2) + I(BASERUN_SB^2) + I(BASERUN_CS^2) + I(PITCHING_H^2) + I(PITCHING_HR^2) + I(PITCHING_BB^2) + I(PITCHING_SO^2) + I(FIELDING_E^2) + I(FIELDING_DP^2) + I(BATTING_1B^2) + I(BATTING_2B^3) + I(BATTING_3B^3) + I(BATTING_HR^3) + I(BATTING_BB^3) + I(BATTING_SO^3) + I(BASERUN_SB^3) + I(BASERUN_CS^3) + I(PITCHING_H^3) + I(PITCHING_HR^3) + I(PITCHING_BB^3) + I(PITCHING_SO^3) + I(FIELDING_E^3) + I(FIELDING_DP^3) + I(BATTING_1B^3) + I(BATTING_2B^4) + I(BATTING_3B^4) + I(BATTING_HR^4) + I(BATTING_BB^4) + I(BATTING_SO^4) + I(BASERUN_SB^4) + I(BASERUN_CS^4) + I(PITCHING_H^4) + I(PITCHING_HR^4) + I(PITCHING_BB^4) + I(PITCHING_SO^4) + I(FIELDING_E^4) + I(FIELDING_DP^4) + I(BATTING_1B^4)"

full_model <- lm(full_formula, train_data)
step_back_model <- stepAIC(full_model, direction="backward", trsace = F)
poly.call <- summary(step_back_model)$call
final_model <- lm(poly.call[2], train_data)
summary(final_model)

hold_out_data$Prediction <- predict(final_model,hold_out_data)
cor(hold_out_data$Prediction,hold_out_data$TARGET_WINS)

```

