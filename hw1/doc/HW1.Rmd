---
title: "DATA 621 Homework #1"
author: "Critical Thinking Group 3"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    toc_depth: 3
    code_folding: "hide"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(tidyverse)
library(kableExtra)
library(corrplot)
library(caret)
```

## Introduction

```{r read_data, echo=FALSE}
# Read in the training data
training <- read.csv("../data/moneyball-training-data.csv") %>%
  select(-INDEX) # Dropping meaningless index
# Read in the evaluation data
evaluation <- read.csv("../data/moneyball-evaluation-data.csv")
```

We have been given a dataset with `r nrow(training)` records summarizing a major league baseball team's season. The records span 1871 to 2006 inclusive.  All statistics have been adjusted to match the performance of a 162 game season.  The objective is to build a linear regression model to predict the number of wins for a team.

### Working Theory

We are working on the premise that there are "good" teams and there are "bad" teams.  The good teams win more than the bad teams.  We are assuming that some of the predictors will be higher for the good teams than for the bad teams.  Consequently we can use these variables to predict how many times a team will win in a season.

### Notes About the Data

There are some difficulties with this dataset.  First it covers such a wide time period.  We know there are different "eras" of baseball.  This data will span multiple eras.  Has the fundamental relationships between wining and these predictors change over time?  We think it has.  If so this will be a challenge.

## Data Exploration

### First Look at the Data

We will first look at the data to get a sense of what we have.

```{r small_multiples_density, warning=FALSE}
training %>%
  gather(variable, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_density(fill = "indianred4", color="indianred4") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())
```

```{r}
quick_summary <- function(df){
  df %>%
    summary() %>%
    kable() %>%
    kable_styling()
}

quick_summary(training)
```

Some initial observations:  

* The response variable (`TARGET_WINS`) looks to be normally distributed.  This supports the working theory that there are good teams and bad teams.  There are also a lot of average teams.
* There are also quite a few variables with missing values.  We may need to deal with these in order to have the largest data set possible for modeling.
* A couple variables are bimodal (`TEAM_BATTING_HR`, `TEAM_BATTING_SO` `TEAM_PITCHING_HR`).  This may be a challenge as some of them are missing values and that may be a challenge in filling in missing values.
* Some variables are right skewed (`TEAM_BASERUN_CS`, `TEAM_BASERUN_SB`, etc.).  This might support the good team theory.  It may also introduce non-normally distributed residuals in the model.  We shall see.  

### Correlations

Let's take a look at the correlations.  The following is the correlations from the complete cases only:

```{r correlation plot}
training %>% 
  cor(., use = "complete.obs") %>%
  corrplot(., method = "color", type = "upper", tl.col = "black", diag = FALSE)
```

```{r}
temp <- training %>% 
  cor(., use = "complete.obs") #%>%
  
temp[lower.tri(temp, diag=TRUE)] <- ""
temp <- temp %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  gather(Variable, Correlation, -rowname) %>%
  filter(Variable != rowname) %>%
  filter(Correlation != "") %>%
  mutate(Correlation = as.numeric(Correlation)) %>%
  rename(` Variable` = rowname) %>%
  arrange(desc(abs(Correlation))) 
```


#### Correlations with Response Variable

Let's take a look at how the predictors are correlated with the response variable:

```{r warning=FALSE}
training %>%
  gather(variable, value, -TARGET_WINS) %>%
  ggplot(., aes(value, TARGET_WINS)) + 
  geom_point(fill = "indianred4", color="indianred4") + 
  geom_smooth(method = "lm", se = FALSE, color = "black") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = "Wins")
```

```{r}
temp %>%
  filter(` Variable` == "TARGET_WINS") %>%
  kable() %>%
  kable_styling()
```

It looks like the hits, walks, home runs, and errors have the strongest correlations with wins.  None of these correlations are particularly strong.  This suggests there is a lot of 'noise' in these relationships.

It is interesting to note allowing hits is positively correlated with wins. How strange! It is also noteworthy that pitching strikeouts is negatively correlated with winning.  That does not make any sense.  When one examines the scatter plots above it becomes apparent that these correlations are being effected by some outliers.

#### Strong Correlations (Absolute Value > 0.5)

Are any predictors are correlated with each other?  We will only look for "strong" correlations:

```{r}
temp %>%
  filter(abs(Correlation) > .5) %>%
  kable() %>%
  kable_styling()
```

There are `r temp %>% filter(Correlation > .99) %>% nrow(.)` variables that have a correlation that is almost 1!  We will need to be careful to prevent adding autocorrelation errors to our model.

### Missing Values

During our first look at the data it was noted that there were variables that are missing data.  Here's a look at what variables are missing data and how big of a problem is it:

```{r}
training %>% 
  gather(variable, value) %>%
  filter(is.na(value)) %>%
  group_by(variable) %>%
  tally() %>%
  mutate(percent = n / nrow(training) * 100) %>%
  mutate(percent = paste0(round(percent, ifelse(percent < 10, 1, 0)), "%")) %>%
  arrange(desc(n)) %>%
  rename(`Variable Missing Data` = variable,
         `Number of Records` = n,
         `Share of Total` = percent) %>%
  kable() %>%
  kable_styling()
```

The hit by pitcher varriable is missing over 90% of it's data.  Based on the weak correlation with the response variable we will exclude it from consideration in our model.

Caught stealling a base (`TEAM_BASERUN_CS`) is next on the list.  It may be possible to predict it using `TEAM_BASERUN_SB` since they are strongly correlated, but there are `r training %>% filter(is.na(TEAM_BASERUN_SB) & is.na(TEAM_BASERUN_SB)) %>% nrow()` times they both are missing data.

The strike outs are going to be a little tricky because of their bimodal distribution.  All `r training %>% filter(is.na(TEAM_BATTING_SO) & is.na(TEAM_PITCHING_SO)) %>% nrow()`


### Zero Values

There are also variables that have verly low values.  Let's see how big of a problem this is:

```{r}
training %>% 
  gather(variable, value) %>%
  filter(value == 0) %>%
  group_by(variable) %>%
  tally() %>%
  mutate(percent = n / nrow(training) * 100) %>%
  mutate(percent = paste0(round(percent, ifelse(percent < 10, 1, 0)), "%")) %>%
  arrange(desc(n)) %>%
  rename(`Variable With Zeros` = variable,
         `Number of Records` = n,
         `Share of Total` = percent) %>%
  kable() %>%
  kable_styling()
```

This isn't nearly as large of a problem as the missing values.  Now it's time to fix the data issues.

## Data Preparation

### Fixing Missing/Zero Values

#### TARGET_WINS

```{r}
training %>% 
  ggplot(aes(TARGET_WINS)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TARGET_WINS)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TARGET_WINS)), col = "green", lty = 2) +
  labs(x = element_blank(), 
       y = "Count",
       title = "Distribution of Wins",
       caption = "* Red line is the mean value and green is the median")
```

The range of data looks good except for a single zero value. Since there are no recorded seasons with zero wins in MLB, we should impute a value in it's place. Here the mean (`r round(mean(training$TARGET_WINS), 0)`) seems as good as any value, so we will replace it.

```{r}
# Replace 0 with the mean value
training[which(training$TARGET_WINS == 0), "TARGET_WINS"] <- round(mean(training$TARGET_WINS), 0)
```

#### TEAM_BATTING_3B

This field represents triples hit by the team. Triples aren't very common because the ball is still in the field of play (unlike a homerun) but the batter still has enough time to get 3 bases.

```{r}
training %>% 
  ggplot(aes(TEAM_BATTING_3B)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TEAM_BATTING_3B)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_BATTING_3B)), col = "green", lty = 2) +
  labs(x = element_blank(), 
       y = "Count",
       title = "Distribution of Triples",
       caption = "* Red line is the mean value and green is the median")
```

Looking at the distribution, the value of zero doesn't look too unusual. Even if it were, the value is not likely to have a large impact.

#### TEAM_BATTING_HR

Although homeruns are more common in modern baseball (thank you PDEs!), there are some low values in the data. So zero doesn't seem too unusual here either.

```{r}
training %>% 
  ggplot(aes(TEAM_BATTING_HR)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TEAM_BATTING_HR)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_BATTING_HR)), col = "green", lty = 2) +
  labs(x = element_blank(), 
       y = "Count",
       title = "Distribution of Homeruns",
       caption = "* Red line is the mean value and green is the median")
```

#### TEAM_BATTING_BB

This variable represents when the batter is "walked" by the pitcher (also known as Base on Balls):

```{r}
training %>% 
  ggplot(aes(TEAM_BATTING_BB)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TEAM_BATTING_BB)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_BATTING_BB)), col = "green", lty = 2) +
  labs(x = element_blank(), 
       y = "Count",
       title = "Distribution of Walks (Base on Balls)",
       caption = "* Red line is the mean value and green is the median")
```

Four balls will walk a batter in modern baseball, however that wasn't always the case. A century or more ago (within the date range of this data set) walks took as many as 9 balls to happen[^1]. Knowing this, and looking at the left-tail of the values above, it is not unreasonable that there might be a season with no walks. Like triples above, leaving the one zero data point in is unlikely to adversely impact any regression, since there are valid values nearby.





#### TEAM_BATTING_SO

Here we saw some NA values, `r training %>% filter(is.na(TEAM_BATTING_SO)) %>% nrow(.)` of them to be specific. Plus we have `r training %>% filter(TEAM_BATTING_SO == 0) %>% nrow(.)` zero values as well.

```{r}
training %>% 
  ggplot(aes(TEAM_BATTING_SO)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TEAM_BATTING_SO,na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_BATTING_SO, na.rm = T)), col = "green", lty = 2) +
  labs(x = element_blank(), 
       y = "Count",
       title = "Distribution of Strikeouts (Batter)",
       caption = "* Red line is the mean value and green is the median")
```

First, the zero values seem nigh-impossible. An entire season (162 games) without a single batter being struck out seems highly suspect, let alone 20 of them in the dataset.

We can replace these values with imputed values, but the distribution looks to be bimodal, so using a mean or median (which is squarely between those peaks) may cause some issues with the model. So, instead, we will impute values using regression.

We can impute a value for this variable by looking at it's nearest neighbors (based on other variables) and taking a weighted average of their values.

```{r}
# Change 0's to NA so they too can be imputed
training <- training %>%
  mutate(TEAM_BATTING_SO = ifelse(TEAM_BATTING_SO == 0, NA, TEAM_BATTING_SO))
```


Looking at our distribution, the shape hasn't changed radically and the means and medians ony shifted slightly:

```{r}
training %>% 
  ggplot(aes(TEAM_BATTING_SO)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TEAM_BATTING_SO, na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_BATTING_SO, na.rm = T)), col = "green", lty = 2) +
  labs(x = element_blank(),
       y = "Count",
       title = "Distribution of Strikeouts (Batter)",
       subtitle = "After Imputation",
       caption = "* Red line is the mean value and green is the median")
```

#### TEAM_BASERUN_SB

With this variable, we have a good number of NA values, and 2 zeroes:

```{r}
training %>% 
  ggplot(aes(TEAM_BASERUN_SB)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TEAM_BASERUN_SB, na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_BASERUN_SB, na.rm = T)), col = "green", lty = 2) +
  labs(x = element_blank(),
       y = "Count",
       title = "Distribution of Stolen Bases",
       caption = "* Red line is the mean value and green is the median")
```

The zeroes may be legitimate here so we will leave them alone. For the NAs, we can use the same KNN imputation we used above for strikeouts

```{r}
#cleaned$TEAM_BASERUN_SB <- temp$TEAM_BASERUN_SB

training %>% 
  ggplot(aes(TEAM_BASERUN_SB)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TEAM_BASERUN_SB, na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_BASERUN_SB, na.rm = T)), col = "green", lty = 2) +
  labs(x = element_blank(),
       y = "Count",
       title = "Distribution of Stolen Bases",
       subtitle = "After Imputation",
       caption = "* Red line is the mean value and green is the median")
```

Again we see little structural change in our distribution.

#### TEAM_BASERUN_CS

This variable is NA for nearly a third of records and only 2 zero values (which could be legitimate values):

```{r}
training %>% 
  ggplot(aes(TEAM_BASERUN_CS)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TEAM_BASERUN_CS, na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_BASERUN_CS, na.rm = T)),col = "green", lty = 2) +
  labs(x = element_blank(),
       y = "Count",
       title = "Distribution of Caught Stealing Bases",
       caption = "* Red line is the mean value and green is the median")
```

Despite the high number of missing values (and a potential for increased error), we will use the KNN imputed values.

```{r}
#cleaned$TEAM_BASERUN_CS <- temp$TEAM_BASERUN_CS
```

#### TEAM_BATTING_HBP

With this variable, we see nearly all entries are missing:

```{r}
training %>% 
  ggplot(aes(TEAM_BATTING_HBP)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TEAM_BATTING_HBP, na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_BATTING_HBP, na.rm = T)), col = "green", lty = 2) +
  labs(x = element_blank(),
       y = "Count",
       title = "Distribution of Being Hit By a Pitch",
       caption = "* Red line is the mean value and green is the median")
```

We *could* make an assumption that these are all cases where there were no batters hit by a pitch, but that seems ill-advised given the distribution above. In this case, we might be wise to leave them as NA for now.

#### TEAM_PITCHING_HR

This variable has no NA values, but there are a few zero values. However, the zero values seem to be legitimate given the distribution:

```{r}
training %>% 
  ggplot(aes(TEAM_PITCHING_HR)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TEAM_PITCHING_HR, na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_PITCHING_HR, na.rm = T)), col = "green", lty = 2) +
  labs(x = element_blank(),
       y = "Count",
       title = "Distribution of Homeruns Pitched",
       caption = "* Red line is the mean value and green is the median")
```

#### TEAM_PITCHING_BB

Here we have no NA values and a single zero:

```{r}
training %>% 
  ggplot(aes(TEAM_PITCHING_BB)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TEAM_PITCHING_BB, na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_PITCHING_BB, na.rm = T)), col = "green", lty = 2) +
  labs(x = element_blank(),
       y = "Count",
       title = "Distribution of Walks Pitched",
       caption = "* Red line is the mean value and green is the median")
```

As we did with walks above, we can assume that is is possible to have no walks (and therefore pitch no walks). So, we will leave the zero alone.

However, there are some **really** high values in the data, which strains reality a little. We could take anything defined as an outlier ($1.5 \cdot \text{IQR}$) and set it to NA so those records will be excluded from any model we build with this variable. But, when you do the math it seems extreme, but plausible. For example, the most number of games in a season in MLB is 162 (currently). With a max value or 3,645 walks pitched you get 22.5 walks per game on average. Divided equally amongst 9 innings, it comes out to 2.5 walks per inning. 

I'd be surprised that any pitcher wouldn't be removed after an inning or two of 2-3 walks, but neither can we rule it out as a possibility.

#### TEAM_PITCHING_SO

This variable represents strikeouts pitched. We see that there are 102 NA values and a *lot* of extremely high values:

```{r}
training %>% 
  ggplot(aes(TEAM_PITCHING_SO)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TEAM_PITCHING_SO, na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_PITCHING_SO, na.rm = T)), col = "green", lty = 2) +
  labs(x = element_blank(),
       y = "Count",
       title = "Distribution of Strikeouts Pitched",
       caption = "* Red line is the mean value and green is the median")
```

We can use the KNN imputation to help with the NA values

```{r}
#cleaned$TEAM_PITCHING_SO <- temp$TEAM_PITCHING_SO
```

With the NA's fixed, we can turn our attention now to the extreme values.

The extreme values need to be handled. First, a typical game will be 9 innings in length, and in each inning you can only pitch 3 strikeouts (because then your part of the inning is over). Those 27 potential strikeouts multiplied by 162 games means an upper limit near 4,374 a season.

Games can go beyond 9 innings, but even if every game in a season was as long as the longest ever MLB game (26 innings) you can only have 12,636 strikeouts. So, the max value of `r max(training$TEAM_PITCHING_SO,na.rm=T)` is invalid.

We'll make a high-yet-reasonable assumption of a mean 11 innings per game, and call anything more than 5,346 strikeouts an invalid data point by setting them to NA so they will be disregarded in any modeling.

```{r}
#cleaned[which(cleaned$TEAM_PITCHING_SO > 5346),"TEAM_PITCHING_SO"] <- NA
```

```{r}
training %>% 
  ggplot(aes(TEAM_PITCHING_SO)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TEAM_PITCHING_SO, na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_PITCHING_SO, na.rm = T)), col = "green", lty = 2) +
  labs(x = element_blank(),
       y = "Count",
       title = "Distribution of Strikeouts Pitched",
       subtitle = "After Removing Invalid Data and Imputation",
       caption = "* Red line is the mean value and green is the median")
```

#### TEAM_FIELDING_DP

The values in this variable seem reasonable, however we do have some NA values.

```{r}
training %>% 
  ggplot(aes(TEAM_FIELDING_DP)) + 
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = mean(TEAM_FIELDING_DP, na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_FIELDING_DP, na.rm = T)), col = "green", lty = 2) +
  labs(x = element_blank(),
       y = "Counts",
       title = "Distribution of Double Plays",
       caption = "* Red line is the mean value and green is the median")
```

Again, we use the KNN imputation from earlier to fill in NAs with imputed values.

```{r}
#cleaned$TEAM_FIELDING_DP <- temp$TEAM_FIELDING_DP
```





```{r}
#training  <- training %>%
#  na.omit()
  #select(-TEAM_BATTING_HBP)
# set.seed(1)
# fix_missing_vals <- training %>%
#   na.omit() %>%
#   preProcess(., "knnImpute")
 
# training_mean <- mean(training$TARGET_WINS)
# 
# training <- predict(fix_missing_vals, training)
# 
# training$TEAM_FIP <- (training$TEAM_PITCHING_HR * 13) + (3 * training$TEAM_PITCHING_BB) - (2 * training$TEAM_PITCHING_SO)
# 
# pp <- preProcess(training, method = c("BoxCox", "center", "scale"))
# training <- predict(pp, training)
```

```{r eval=FALSE, echo=FALSE}
library(rpart)
library(rpart.plot)
m1 <- rpart(TARGET_WINS ~ ., data = training, method = "anova")
rpart.plot(m1)
plotcp(m1)
```

```{r eval=FALSE, echo=FALSE}
# Specify 10-fold cross validation
ctrl <- trainControl(method = "cv",  number = 10) 

# CV bagged model
bagged_cv <- training %>%
  na.omit() %>%
  train(
    TARGET_WINS ~ ., 
    data = .,
    method = "treebag",
    trControl = ctrl,
    importance = TRUE
  )

# assess results
bagged_cv

plot(varImp(bagged_cv)) 

imp <- varImp(bagged_cv)$importance %>% 
  data.frame() %>%
  rownames_to_column()
```

## Model Building

## Model Selection

```{r}
set.seed(42)
train_index <- createDataPartition(training$TARGET_WINS, p = .8, list = FALSE, times = 1)

moneyball_train <- training[train_index,]
moneyball_test <- training[-train_index,]
```

```{r}
fit <- lm(TARGET_WINS ~ ., moneyball_train)
summary(fit)
```

```{r}
moneyball_test$y_hat <- predict(fit, moneyball_test)
moneyball_test <- moneyball_test %>%
  mutate(error = TARGET_WINS - y_hat) %>%
  mutate(squared_error = error ^ 2)

ggplot(moneyball_test, aes(error))+
  geom_histogram(bins = 50)

summary(moneyball_test$squared_error)
```

## Appendix
