---
title: "DATA 621 Homework #3"
author: "Critical Thinking Group 3"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    toc_depth: 3
    code_folding: "hide"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(tidyverse)
library(kableExtra)
library(corrplot)
library(caret)
library(Amelia)
# Thank you Stack Overflow!
# A Prefix nulling hook.

# Make sure to keep the default for normal processing.
default_output_hook <- knitr::knit_hooks$get("output")

# Output hooks handle normal R console output.
knitr::knit_hooks$set( output = function(x, options) {

  comment <- knitr::opts_current$get("comment")
  if( is.na(comment) ) comment <- ""
  can_null <- grepl( paste0( comment, "\\s*\\[\\d?\\]" ),
                     x, perl = TRUE)
  do_null <- isTRUE( knitr::opts_current$get("null_prefix") )
  if( can_null && do_null ) {
    # By default R print output aligns at the right brace.
    align_index <- regexpr( "\\]", x )[1] - 1
    # Two cases: start or newline
    re <- paste0( "^.{", align_index, "}\\]")
    rep <- comment
    x <- gsub( re, rep,  x )
    re <- paste0( "\\\n.{", align_index, "}\\]")
    rep <- paste0( "\n", comment )
    x <- gsub( re, rep,  x )
  }

  default_output_hook( x, options )

})
knitr::opts_template$set("kill_prefix"=list(comment=NA, null_prefix=TRUE))
```

```{r}
df <- read.csv('data/crime-training-data_modified.csv')
evaluation <- read.csv("data/crime-evaluation-data_modified.csv")
```

## Data Exploration

### Are There Missing Values?
First we look at the data to see if any variables have missing data:

```{r}
missmap(df, main = "Missing vs Observed Values")
```

It looks like we have a complete data set.  No need to impute values.

### Splitting the Data
Next, we look to split our data between a training set (`train`) and a test data set (`test`). We'll use a 70-30 split between train and test, respectively.
```{r}
set.seed(42)
train_index <- createDataPartition(df$target, p = .7, list = FALSE, times = 1)
train <- df[train_index,]
test <- df[-train_index,]
```


### Exploratory Data Analysis
Now, let's look at our training data. By looking at a correlation matrix, we can see which variables may be too correlated to be included together in a model as predictor variables. This will help us later during the model selection process.

```{r}
train %>% 
  cor(.) %>%
  corrplot(., method = "color", type = "upper", tl.col = "black", diag = FALSE)
```

Next we will look at each potential predictor and how it is distributed across the target variable.

The following plots show how predictors are distributed between a positive target variable (areas with crime rates higher than the median, i.e. blue) and a negative target variable (areas with crime rates below the median, i.e. red). What we are looking for is variables that show way to split data into two groups.

```{r}
for (var in names(train)){
  if(var != "target"){
    plot_df <- train
    plot_df$x <- plot_df[,var]
    p <- ggplot(plot_df, aes(x, color = factor(target))) +
      geom_density() +
      theme_light() +
      ggtitle(var) +
      scale_color_brewer(palette = "Set1") +
      theme(legend.position = "none",
            axis.title.y = element_blank(),
            axis.title.x = element_blank())
    print(p)
  }
}
```

Looking at the plots above, the `nox` variable seems to be the best variable to divide the data into the two groups.

## Data Preparation

## Model Building

We start by applying occam's razor and create a baseline model that only has one predictor. Any model we build beyond that will have to outperform this simplest model.

```{r}
baseline <- glm(target ~ nox, family = binomial(link = "logit"), train)
summary(baseline)
test$baseline <- ifelse(predict.glm(baseline, test,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(test$baseline), factor(test$target),"1")
results <- tibble(model = "baseline",predictors = 1,F1 = cm$byClass[7],
                  deviance=baseline$deviance, 
                  r2 = 1 - baseline$deviance/baseline$null.deviance,
                  aic=baseline$aic)
cm
```

Our baseline model is ok with an F1 score of `r cm$byClass[7]`.

Next we try adding every other variable, to build a full model. From here we can work backwards and eliminate non-significant predictors:

```{r}
fullmodel <- glm(target ~ ., family = binomial(link = "logit"), train)
summary(fullmodel)
test$fullmodel <- ifelse(predict.glm(fullmodel, test,"response") < 0.5, 0, 1)
cm <- confusionMatrix(factor(test$fullmodel), factor(test$target),"1")
results <- rbind(results,tibble(model = "fullmodel",
                                predictors = 12,F1 = cm$byClass[7],
                                deviance=fullmodel$deviance, 
                                r2 = 1-fullmodel$deviance/fullmodel$null.deviance,
                                aic=fullmodel$aic))
cm
```

The full model has an F1 score is `r cm$byClass[7]`, which is a bit higher than before. However, many variables do not seem to be significant.

After some backward elimination of non-significant predictor variables, we arrive at the following model: 

```{r}
model1 <- glm(target ~ . -tax -rm -chas - age -zn -indus, 
              family = binomial(link = "logit"), 
              train)
summary(model1)
test$model1 <- ifelse(predict.glm(model1, test,"response") < 0.5, 0, 1)
cm <- confusionMatrix(factor(test$model1), factor(test$target),"1")
results <- rbind(results,tibble(model = "model1",
                                predictors = 6,F1 = cm$byClass[7],
                                deviance=model1$deviance, 
                                r2 = 1-model1$deviance/model1$null.deviance,
                                aic=model1$aic))
cm
```

Since the assignment mentions that the purpose is prediction, we will prefer F1 score as our measure of model success.

```{r}
kable(results)
```

Looking these three models together, `model1` looks like the best one. Although the full model scored an F1 that was ever-so-slightly higher on our test data set, `model1` has half the predictors and it's scores are almost exactly the same as `fullmodel`.
